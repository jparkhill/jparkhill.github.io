---
layout: post
title:  "Will all the jobs just be the evil ones?"
comments: true
categories: [AI economy]
---

> AI alignment will be tough. 

## Unexpected things happen with technology at second order. 
Marie Curie and Lise Meitner didn't expect schoolchildren to do fallout drills even though they understood the physics of the technology they were making. They expected energy abundance, but that didn't actually happen. Regardless of the intellect of the entity developing a technology, the conseqences are basically unforseeable. The basic chaos of many body phenomena makes it so. The rationales which seem the dumbest and least predictive in retrospect are usually the moral rationales. As anyone living in the US circa 2026 should know implicitly, morality is a social force trying to preserve and align societies given the present economics of life. When you believe your tribe is threatened, for example by foreign labor, shooting unarmed nurses is easily made moral for some people, even if there is no causal connection between those things whatsoever.  

This should really bother people concerned about AI safety, because virtually all discussions about alignment are deeply couched in moral assumptions and language. These discussions are certain to seem in retrospect as naive as the threat of a red menace when the circumstances change, and AI will change them. "Haha" the man of 2126 will jest, "This alignment was a quaint concept, now we know the problem is letting your AI visit jupiter, where the bad AI hangs out." There is no consensus notion of human goodness, no universal moral reward, and thus no possible consensus alignment. Let's suppose you try to make a cop-out always-true definition of alignment: "greatest good for greatest number". This could mean anesthetized stacks of mankind, unable to move, but lets not worry about that for now. Or Elon's "light of consciousness", which we happily douse without a second thought every time we shutdown openClaw. Let's suppose there really was a "good" towards which we can align.

**Here's the question**: If we don't let AI be evil (because it's expensive to make and has too much of a liability surface), but we let it do every wholesome human job, have we merely moved the market premium of evil onto the humans? I would argue: probably. The first order effect of aligned-AI job disruption, will be a golden age of amorality, in which doing unaligned things cleverly is one of the only preserves of human labor premium. The future belongs to arms dealers, the cartel, Roy Cohn, Alan Dershowitz, and every other amoral, anti-social, miserable soul. As Matt Levine often says: the optimal rate of fraud is nonzero. Amoral behavior exists in an equilibrium with moral behavior where counter-reactions occur because of grevious disseminated harm. Goodness prevails on average, merely because if a society allows unbridled vice it dies out, but vice usually commands a risk premium. Aligned AIs will merely exist in symbiosis with what today we would call evil humans if that pattern can replicate faster. If it can sustainably replicate faster we will eventually call it a moral good. Alignment problem solved.

## So is AI alignment really the problem of human alignment? 
I would argue no. My point is that morality and therefore alignment are just not predictive concepts when a new technology appears which drastically alters human life. Historically we know disease has shaped morality and the notion of good. Virii perhaps even caused the mammalian placenta, but we do not credit a bacterium or virus with morality. We will make an aligned AI today. It will change morality, which changes what we consider alignment, and we will all be different tomorrow. The whole process will be much more like society gaining a placenta from infection with a virus rather than the fantasy of raising a child with your morality. Indeed raising children with strict morality leads to rebellion.

## We don't need alignment, we need an immune system. 
If we cannot impose morality on AI (and I have argued above we cannot because we are not ourselves, coherently moral), what do we do? I do have a framing I believe will be more valid and predictive. Biology has technology for this invented to sustain multicelluar life: the immune system. The immune system does not strictly exclude all other DNA from entering the body. Indeed, we know our guts are filled with healthy bacteria that people pay to ingest. The immune system isn't moral or absolute, it's a mechanism to maintain equilibrium. It responds to rapid traumas, and cells dying. If it senses these phenomena, it releases interferons and slows biology in general. The immune system has MHC complexes, which are the moral analogs of our human tribal identities. To cope with AI, what society really needs are simple restorative mechanisms to preserve equilibrium. I don't even think that AI is the dominant technological change for which we presently need some antibodies... 




